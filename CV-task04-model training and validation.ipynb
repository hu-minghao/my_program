{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练及验证\n",
    "\n",
    "上一节搭建了字符识别模型，接下来就是模型的训练及验证了，这里一般分为两个部分。第一部分是模型的训练，然后是测试集上测试其泛化性能。\n",
    "\n",
    "## 1.模型训练\n",
    "\n",
    "这里按数据集使用来划分，一般会将数据集拆分为三块，分别为训练数据train data,验证数据 val data 和测试数据test data。在训练模型时，模型参数的学习使用train data来学习。一般随着训练批次的增加，模型的train loss，即训练损失会越来越小，这时候模型对train数据集的特征学习的越来越好，但是会在没有训练的数据集上的误差会越来越大。这里的val data，是在训练过程中用来验证模型的泛化性能的数据集。如果模型评分在val data上变差，说明开始出现过拟合，此时需要停止训练。val data对我们模型的训练，调参具有很好的指导作用。\n",
    "\n",
    "## 2.模型测试\n",
    "\n",
    "训练完毕的模型，在之前完全没有参入模型训练的数据集上进行预测，得到评分，这个叫模型的测试阶段，因为test data模型完全没有接触过，所以可以测试模型的泛化性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集的准备\n",
    "\n",
    "模型训练和验证之前，第一步是准确数据，这里需要用到pytorch的dataset 和dataloder类，分别构建数据读取及数据加载函数。\n",
    "\n",
    "## 1. dataset \n",
    "\n",
    "这里使用pytorch的dataset类，继承其属性，构建我们自己的dataset\n",
    "\n",
    "pythorch的dataset类在torch.utils.data.dataset的Dataset中，读取数据时，我们keys使用图片处理的方法进行数据增强，在torchvision中有数据增强的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil, json\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torch\n",
    "#torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义自己的数据集类,从Dataset处继承\n",
    "class MyDataset(Dataset):\n",
    "    #定义初始化方法\n",
    "    def __init__(self,img_path,img_label,transform=None):\n",
    "        #三个参数，图片路径，图片标签和转化方式，转化方式与数据集扩展有关\n",
    "        self.img_path=img_path\n",
    "        self.img_label=img_label\n",
    "        #注意我们可以先不进行数据增强，所以要加判断\n",
    "        if transform !=None:\n",
    "            self.transform=transform\n",
    "        else:\n",
    "            self.transform=None\n",
    "    \n",
    "    #构建数据集需要两个方法，一个是getitem,一个是len,这里我觉得getitem就是调用这个datasset后，会一个一个返回数据和标签的方法\n",
    "    #len会返回数据集大小，这个getitem是按编号返回数据及标签。\n",
    "    \n",
    "    #这里的open是打开一个文件夹，注意，这里文件是一个一个读取的，img_path是一个list!所以有索引！！！，同理，label——path\n",
    "    #也是一样.\n",
    "    def __getitem__(self, index):\n",
    "        #读取图片的方法可以换\n",
    "        img = Image.open(self.img_path[index]).convert('RGB')\n",
    "    #转换img\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # 设置最长的字符长度为5个,读取label,注意这里转换了一下数据类型\n",
    "        lbl = np.array(self.img_label[index], dtype=np.int)\n",
    "        #由于是固定长度，所以将不足5位的地方补足了\n",
    "        lbl = list(lbl)  + (5 - len(lbl)) * [10]\n",
    "        return img, torch.from_numpy(np.array(lbl[:5]))\n",
    "   #这里再次注意img_path是一个列表\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Dataloder\n",
    "\n",
    "### 接下来就可以用DataLoder来加载定义好的Dataset了，需要我们指定好batch_size，即每次调用数据集的数量，是否打乱操作，和是否多线程。如果是window，则num_workers需要设置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000\n"
     ]
    }
   ],
   "source": [
    "#glob.glob读取结果是一个列表，类似于linnix的查找文件方式，可迭代对象\n",
    "train_path = glob.glob('E:/Machine Learning/CV字符/mchar_train/mchar_train/*.png')\n",
    "#列表排序\n",
    "train_path.sort()\n",
    "#json.load()是用来读取文件的，即，将文件打开然后就可以直接读取。示例如下\n",
    "#with open(\"文件名\") as f:\n",
    "#    result=json.load(f)\n",
    "train_json = json.load(open('E:/Machine Learning/CV字符/mchar_train.json'))\n",
    "#json文件格式类似于字典，下面是一个试例\n",
    "#{\"000000.png\": {\"height\": [219, 219], \"label\": [1, 9], \"left\": [246, 323], \"top\": [77, 81], \"width\": [81, 96]},\n",
    "train_label = [train_json[x]['label'] for x in train_json]\n",
    "print(len(train_path), len(train_label))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(train_path, train_label,\n",
    "                transforms.Compose([\n",
    "                    transforms.Resize((64, 128)),\n",
    "                    transforms.RandomCrop((60, 120)),\n",
    "                    transforms.ColorJitter(0.3, 0.3, 0.2),\n",
    "                    transforms.RandomRotation(5),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])), \n",
    "    batch_size=40, \n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同理，定义验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "val_path = glob.glob('E:/Machine Learning/CV字符/mchar_val/mchar_val/*.png')\n",
    "val_path.sort()\n",
    "val_json = json.load(open('E:/Machine Learning/CV字符/mchar_val.json'))\n",
    "val_label = [val_json[x]['label'] for x in val_json]\n",
    "print(len(val_path), len(val_label))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    MyDataset(val_path, val_label,\n",
    "                transforms.Compose([\n",
    "                    #transforms.Resize((60, 120)),\n",
    "                    # transforms.ColorJitter(0.3, 0.3, 0.2),\n",
    "                    # transforms.RandomRotation(5),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])), \n",
    "    batch_size=40, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.构建模型\n",
    "\n",
    "### 直接使用renset18作为特征提取模块，这里我其实不太明白什么叫特征提取模块，google一下\n",
    "\n",
    "在本文中，主要是介绍提取中间层的特征，对于特征的提取，可以先把模型的结构输出，不同的模型结构是不一样的；下面拿resnet作为示例；由于pytorch模型很多用到nn.sequential,所以对各层的特征提取要自己去修改forward函数。这里可以看到，我们自定义了forward函数，由此可以选择在哪一层提取特征。\n",
    "\n",
    "大概意思就是，在模型的某一神经层，提取其输出信息。具体哪一层需要在forward里修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHN_Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVHN_Model1, self).__init__()\n",
    "        #这里选择了resnet，这里就需要对其网络结构比较熟悉，才能去相应层提取信息     \n",
    "        model_conv = models.resnet18(pretrained=True)\n",
    "        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        model_conv = nn.Sequential(*list(model_conv.children())[:-1])\n",
    "        self.cnn = model_conv\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 11)\n",
    "        self.fc2 = nn.Linear(512, 11)\n",
    "        self.fc3 = nn.Linear(512, 11)\n",
    "        self.fc4 = nn.Linear(512, 11)\n",
    "        self.fc5 = nn.Linear(512, 11)\n",
    "    \n",
    "    def forward(self, img):        \n",
    "        feat = self.cnn(img)\n",
    "        # print(feat.shape)\n",
    "        feat = feat.view(feat.shape[0], -1)\n",
    "        c1 = self.fc1(feat)\n",
    "        c2 = self.fc2(feat)\n",
    "        c3 = self.fc3(feat)\n",
    "        c4 = self.fc4(feat)\n",
    "        c5 = self.fc5(feat)\n",
    "        return c1, c2, c3, c4, c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练，验证，测试模块 \n",
    "\n",
    "下面定义好训练、验证和预测模块  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#后面的函数，我其实看不太懂了，要赶打卡进度，就先拷贝教材里的内容了\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    # 切换模型为训练模式\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    \n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        c0, c1, c2, c3, c4 = model(input)\n",
    "        loss = criterion(c0, target[:, 0]) + \\\n",
    "                criterion(c1, target[:, 1]) + \\\n",
    "                criterion(c2, target[:, 2]) + \\\n",
    "                criterion(c3, target[:, 3]) + \\\n",
    "                criterion(c4, target[:, 4])\n",
    "        \n",
    "        # loss /= 6\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(loss.item())\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    # 切换模型为预测模型\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    # 不记录模型梯度信息\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            if use_cuda:\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            c0, c1, c2, c3, c4 = model(input)\n",
    "            loss = criterion(c0, target[:, 0]) + \\\n",
    "                    criterion(c1, target[:, 1]) + \\\n",
    "                    criterion(c2, target[:, 2]) + \\\n",
    "                    criterion(c3, target[:, 3]) + \\\n",
    "                    criterion(c4, target[:, 4])\n",
    "            # loss /= 6\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "def predict(test_loader, model, tta=10):\n",
    "    model.eval()\n",
    "    test_pred_tta = None\n",
    "    \n",
    "    # TTA 次数\n",
    "    for _ in range(tta):\n",
    "        test_pred = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(test_loader):\n",
    "                if use_cuda:\n",
    "                    input = input.cuda()\n",
    "                \n",
    "                c0, c1, c2, c3, c4 = model(input)\n",
    "                output = np.concatenate([\n",
    "                    c0.data.numpy(), \n",
    "                    c1.data.numpy(),\n",
    "                    c2.data.numpy(), \n",
    "                    c3.data.numpy(),\n",
    "                    c4.data.numpy()], axis=1)\n",
    "                test_pred.append(output)\n",
    "        \n",
    "        test_pred = np.vstack(test_pred)\n",
    "        if test_pred_tta is None:\n",
    "            test_pred_tta = test_pred\n",
    "        else:\n",
    "            test_pred_tta += test_pred\n",
    "    \n",
    "    return test_pred_tta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据，记录loss值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVHN_Model1()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
    "best_loss = 1000.0\n",
    "\n",
    "use_cuda = False\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "for epoch in range(2):\n",
    "    train_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    val_loss = validate(val_loader, model, criterion)\n",
    "    \n",
    "    val_label = [''.join(map(str, x)) for x in val_loader.dataset.img_label]\n",
    "    val_predict_label = predict(val_loader, model, 1)\n",
    "    val_predict_label = np.vstack([\n",
    "        val_predict_label[:, :11].argmax(1),\n",
    "        val_predict_label[:, 11:22].argmax(1),\n",
    "        val_predict_label[:, 22:33].argmax(1),\n",
    "        val_predict_label[:, 33:44].argmax(1),\n",
    "        val_predict_label[:, 44:55].argmax(1),\n",
    "    ]).T\n",
    "    val_label_pred = []\n",
    "    for x in val_predict_label:\n",
    "        val_label_pred.append(''.join(map(str, x[x!=10])))\n",
    "    \n",
    "    val_char_acc = np.mean(np.array(val_label_pred) == np.array(val_label))\n",
    "    \n",
    "    print('Epoch: {0}, Train loss: {1} \\t Val loss: {2}'.format(epoch, train_loss, val_loss))\n",
    "    print(val_char_acc)\n",
    "    # 记录下验证集精度\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), './model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面就是测试数据集上，验证模型的泛化性能，生成预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = glob.glob('../input/test_a/*.png')\n",
    "test_path.sort()\n",
    "test_label = [[1]] * len(test_path)\n",
    "print(len(val_path), len(val_label))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    SVHNDataset(test_path, test_label,\n",
    "                transforms.Compose([\n",
    "                    transforms.Resize((64, 128)),\n",
    "                    transforms.RandomCrop((60, 120)),\n",
    "                    # transforms.ColorJitter(0.3, 0.3, 0.2),\n",
    "                    # transforms.RandomRotation(5),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])), \n",
    "    batch_size=40, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_predict_label = predict(test_loader, model, 1)\n",
    "\n",
    "test_label = [''.join(map(str, x)) for x in test_loader.dataset.img_label]\n",
    "test_predict_label = np.vstack([\n",
    "    test_predict_label[:, :11].argmax(1),\n",
    "    test_predict_label[:, 11:22].argmax(1),\n",
    "    test_predict_label[:, 22:33].argmax(1),\n",
    "    test_predict_label[:, 33:44].argmax(1),\n",
    "    test_predict_label[:, 44:55].argmax(1),\n",
    "]).T\n",
    "\n",
    "test_label_pred = []\n",
    "for x in test_predict_label:\n",
    "    test_label_pred.append(''.join(map(str, x[x!=10])))\n",
    "    \n",
    "import pandas as pd\n",
    "df_submit = pd.read_csv('E:/Machine Learning/CV字符/test_A_sample_submit.csv')\n",
    "df_submit['file_code'] = test_label_pred\n",
    "df_submit.to_csv('renset18.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
